{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNfDv2RC-iOq"
      },
      "source": [
        "# **negativas**\n",
        "> _versão 2_\n",
        "\n",
        "O presente Notebook tem como objetivo a implementação de um código de busca e classificação de negações sentenciais em arquivos de entrevistas transcritas da amostra _Deslocamentos_, do Banco de Dados Falares Sergipanos¹.\n",
        "\n",
        "O arquivo está disposto da seguinte forma: a **parte 1** trata da instalação dos componentes necessários para a execução do código; a **parte 2** traz a implementação das funções que serão utilizadas pela ferramenta; o código responsável pela análise (busca e classificação) encontra-se na **parte 3**; por fim, a **parte 4** trata do armazenamento dos dados em forma de planilhas.\n",
        "\n",
        "</br>\n",
        "\n",
        "---\n",
        "\n",
        "1. FREITAG, R. M. Ko. Banco de dados falares sergipanos. Working Papers em\n",
        "Linguística, v. 14, n. 2, p. 156-164, 2013. DOI: https://doi.org/10.5007/1984-8420.2013v14n2p156\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yot0_zzT6h3S"
      },
      "source": [
        "> As principais funcionalidades foram desenvolvidas através da utilização do [spaCy](https://spacy.io/usage), biblioteca de Python para Processamento de Linguagem Natural."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFLcPI5E_ip3"
      },
      "source": [
        "## **1. Instalações e importações necessárias**\n",
        "Nessa seção são instaladas e importadas todas as bibliotecas e componentes necessários para o desenvolvimento e funcionamento dos códigos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Vl_OuqbmOL1",
        "outputId": "01ef71bd-28ea-463f-b442-e61511185045"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet spacy\n",
        "!python -m spacy download pt_core_news_lg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Usando o Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSAVowA-mV8E",
        "outputId": "e18cea2b-7887-4bd4-cab7-3940ba3f195f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KFS9e0TmXLk"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "import pandas as pd\n",
        "import string\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sgc8Dp-fAFlE"
      },
      "source": [
        "## **2. Implementando funções**\n",
        "Aqui são implementadas duas funções que farão parte do código principal. Uma para a limpeza do texto e outra para a extração dos metadados de cada entrevista."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0f4QbCMAYmY"
      },
      "source": [
        "### **2.1. Função de limpeza**\n",
        "A função de limpeza irá receber o texto do arquivo em processamento e limpá-lo, removendo stopwords e pontuações."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8X_kTosSmZCx"
      },
      "outputs": [],
      "source": [
        "def limpa_str(texto):\n",
        "    nlp = spacy.blank('pt')\n",
        "    stoplist = ['eh', 'hes', 'RISOS', 'risos', 'BARULHO', 'INTERVENIENTE', 'PIGARRO', 'pigarro', 'CHORO', 'RUÍDO', 'RUÍDOS','est']\n",
        "\n",
        "    texto = texto.encode('utf-8','ignore').decode()\n",
        "    texto = re.sub('[%s]' % re.escape(string.punctuation), ' ', texto)\n",
        "    doc = nlp(texto)\n",
        "    palavras = [palavra for palavra in texto.split() if not re.search([p for p in stoplist], palavra)]\n",
        "    for token in doc:\n",
        "        if token.text not in stoplist:\n",
        "            palavras.append(token.text)\n",
        "    texto_limpo = (' ').join(palavras)\n",
        "\n",
        "    return texto_limpo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZgNpkQXAf0c"
      },
      "source": [
        "### **2.2. Função de extração do cabeçalho**\n",
        "Cada arquivo de entrevista transcrita possui um cabeçalho com dados sobre as pessoas entrevistadas e entrevistadoras. A função a seguir realiza a extração desses dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMW8CXYYAfHd"
      },
      "outputs": [],
      "source": [
        "def extrai_cabecalho(entrevista, dado):\n",
        "    cabecalho_arquivo = []\n",
        "    conteudo = entrevista.split('\\n')\n",
        "\n",
        "    for linha in range (0, 12):\n",
        "        texto = conteudo[linha].strip('\\n').upper().split()\n",
        "        for palavra in range(len(texto)):\n",
        "            documentador = ['DOCUMENTADOR(A):', 'DOCUMENTADOR:']\n",
        "            if texto[palavra] == dado:\n",
        "                return texto[palavra+1]\n",
        "            elif texto[palavra] in documentador:\n",
        "                return texto[palavra+1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVC_UpzMAq0_"
      },
      "source": [
        "## **3. Código de busca e classificação**\n",
        "O código seguinte é o principal da ferramenta, é nele onde estão implementados os comandos de busca e classificação das negações sentenciais.\n",
        "\n",
        "Através dele, todas as entrevistas contidas no diretório serão processadas buscando pelas ocorrências que atendam as regras estabelecidas no `Matcher` da biblioteca spaCy e classificando-as com base nas etiquetas atribuídas à cada padrão, podendo ser:\n",
        "* **pré-verbal**\n",
        "* **pós-verbal**\n",
        "* **dupla negação**\n",
        "\n",
        "Finalizada a busca e classificação, os dados são armazenados em uma lista que dará origem ao DataFrame, contendo as informações do cabeçalho da entrevista, a ocorrência, a classificação da negação e o seu contexto.\n",
        "\n",
        "Além da estrutura contendo os dados analisados, será criado outro DataFrame para armazenar as estatísticas de cada entrevista, como o nº total de negações, quantidade de negações para cada etiqueta e suas porcentagens em relação ao total."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOSaOPtAm9lt"
      },
      "outputs": [],
      "source": [
        "#DataFrame de análise e classificação das negações\n",
        "colunas_neg = [\"Documentador(a)\", \"Inf\", \"Sexo\", \"Ocorrência\", \"Classificação\", \"Contexto\"]\n",
        "linhas_neg = []\n",
        "\n",
        "#DataFrame contendo as estatísticas\n",
        "colunas_est = [\"Arquivo\", \"Inf\", \"Nº de 'nãos'\", \"Negações\", \"Pré-verbal\", \"% pré-verbal\", \"Pós-verbal\", \"% pós-verbal\", \"Dupla negação\", \"% dupla negação\"]\n",
        "linhas_est = []\n",
        "\n",
        "#Lista todos os arquivos contidos na pasta data\n",
        "nomes_arq = !ls 'data'\n",
        "\n",
        "nlp = spacy.blank(\"pt\")\n",
        "nlp = spacy.load(\"pt_core_news_lg\")\n",
        "\n",
        "for nome in nomes_arq:\n",
        "    nome_refatorado = nome.replace(\"'\", \"\")\n",
        "    arquivo =f'data/{nome_refatorado}'\n",
        "\n",
        "    entrevista = open(arquivo, encoding='utf-8').read()\n",
        "\n",
        "    documentador = extrai_cabecalho(entrevista, \"DOCUMENTADORA:\")\n",
        "    inf = extrai_cabecalho(entrevista, \"INF:\")\n",
        "    sexo = extrai_cabecalho(entrevista, \"SEXO:\")\n",
        "\n",
        "    conteudo_arquivo = limpa_str(open(arquivo).read())\n",
        "    doc = nlp(limpa_str(entrevista))\n",
        "\n",
        "    matcher = Matcher(vocab=nlp.vocab)\n",
        "\n",
        "    nao_pre_verbal = [{\"TEXT\": \"não\"}, {\"POS\": {\"IN\": [\"VERB\", \"AUX\"]}}, {\"TEXT\": \"não\", \"OP\": \"!\"}, {\"POS\": {\"NOT_IN\": [\"VERB\", \"AUX\"]}}]\n",
        "    nao_pos_verbal = [{\"TEXT\": \"não\", \"OP\": \"!\"}, {}, {\"POS\": {\"IN\": [\"VERB\", \"AUX\"]}}, {\"TEXT\": \"não\"}]\n",
        "    dupla_negacao = [{\"TEXT\": \"não\"}, {\"POS\": {\"IN\": [\"VERB\", \"AUX\"]}, \"OP\": \"+\"}, {\"TEXT\": \"não\"}, {\"POS\": {\"IN\": [\"VERB\", \"AUX\"]}, \"OP\": \"!\"}]\n",
        "\n",
        "    matcher.add('dupla negação', [dupla_negacao])\n",
        "    matcher.add('pré-verbal', [nao_pre_verbal])\n",
        "    matcher.add('pós-verbal', [nao_pos_verbal])\n",
        "\n",
        "    matches = matcher(doc)\n",
        "\n",
        "    nao = len(re.findall(r'\\s?não\\s?', conteudo_arquivo))\n",
        "    dupla_neg = 0\n",
        "    pre_verbal = 0\n",
        "    pos_verbal = 0\n",
        "    negacoes = 0\n",
        "\n",
        "    for match_id, start, end in matches:\n",
        "        ocorrencia = doc[start:end]\n",
        "        classificacao = nlp.vocab.strings[match_id]\n",
        "        contexto = doc[start-5:end+4]\n",
        "        linhas_neg.append([documentador, inf, sexo, ocorrencia, classificacao, contexto])\n",
        "\n",
        "        if classificacao == 'dupla negação':\n",
        "            dupla_neg += 1\n",
        "            negacoes += 1\n",
        "        elif classificacao == 'pré-verbal':\n",
        "            pre_verbal += 1\n",
        "            negacoes += 1\n",
        "        elif classificacao == 'pós-verbal':\n",
        "            pos_verbal += 1\n",
        "            negacoes += 1\n",
        "\n",
        "    #Armazenando estatísticas básicas\n",
        "    linhas_est.append([nome_refatorado, inf, nao, negacoes, pre_verbal, ((pre_verbal / negacoes) * 100), pos_verbal, ((pos_verbal / negacoes) * 100), dupla_neg, ((dupla_neg / negacoes) * 100)])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAsCM9JBA7FW"
      },
      "source": [
        "## **4. Armazenando os resultados**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDfevO4bBDiz"
      },
      "source": [
        "### **4.1. Construindo e verificando os DataFrames**\n",
        "Aqui, os DataFrames são criados e algumas das suas informações são exibidas para verificar se tudo funcionou corretamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2govN3LdnbxG"
      },
      "source": [
        "**Ocorrências e classificações**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "ASmWuOkBobzC",
        "outputId": "f2687db4-4603-4e50-d513-06ca8511ab3b"
      },
      "outputs": [],
      "source": [
        "df_neg = pd.DataFrame(linhas_neg, columns = colunas_neg)\n",
        "\n",
        "df_neg.info()\n",
        "df_neg.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Estatísticas básicas**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_est = pd.DataFrame(linhas_est, columns = colunas_est)\n",
        "\n",
        "df_est.info()\n",
        "df_est.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_MySll9BJJd"
      },
      "source": [
        "### **4.2. Salvando os DataFrames**\n",
        "O salvamento dos dados pode ocorrer tanto em arquivo `.csv` quanto arquivo excel, `.xlsx`, e o nome do seu arquivo pode ser alterado na váriavel `NOME_ARQUIVO`.\n",
        "\n",
        "> **Observação:** ao salvar os dados, lembre de movê-los para uma pasta de sua preferência em seu Drive ou baixá-lo na sua máquina, pois quando o Google Colab encerra a conexão, os arquivos salvos no ambiente são perdidos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaBSGXORntAV"
      },
      "source": [
        "**Ocorrências e classificações**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFHURVNzBnFK"
      },
      "outputs": [],
      "source": [
        "from datetime import date\n",
        "NOME_ARQUIVO = 'negativas_amostra2020'\n",
        "\n",
        "#Salva em .csv\n",
        "df_neg.to_csv(f\"teste_{date.today().strftime('%d-%m-%Y')}_{NOME_ARQUIVO}.csv\", encoding='UTF-8')\n",
        "\n",
        "#Salva em excel\n",
        "#df_neg.to_excel(f\"{NOME_ARQUIVO}.xlsx\", encoding='UTF-8')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Estatísticas**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Salva em .csv\n",
        "df_est.to_csv(f\"Estatísticas - {NOME_ARQUIVO}.csv\", encoding='UTF-8')\n",
        "\n",
        "#Salva em excel\n",
        "#df_est.to_excel(f\"Estatísticas - {NOME_ARQUIVO}.xlsx\", encoding='UTF-8')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
